{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd040211611db766dc04d4e34028715744ed9c5ebf71c9517b1ff2182e4424a3492",
   "display_name": "Python 3.8.8 64-bit ('rapids-0.19': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8ac587500089045bda0d8680467a78b2a5f7abf79b6086e96a7d36f8c178cf35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import unetnc\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "DATA_PATH = 'Dataset Preview/Inv3D preview/data/'\n",
    "# warped WC sind die 3D Koordinaten nach dem ersten Modell bzw. dewarpnet -> Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainimage_gt = np.load(DATA_PATH + 'train/06/warped_WC.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = trainimage_gt['warped_WC']\n",
    "#img_t = torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "#normalize label\n",
    "lbl = img_t\n",
    "msk=((lbl[:,:,0]!=0)&(lbl[:,:,1]!=0)&(lbl[:,:,2]!=0)).astype(np.uint8)*255\n",
    "xmx, xmn, ymx, ymn,zmx, zmn= 1.2539363, -1.2442188, 1.2396319, -1.2289206, 0.6436657, -0.67492497   # calculate from all the wcs\n",
    "lbl[:,:,0]= (lbl[:,:,0]-zmn)/(zmx-zmn)\n",
    "lbl[:,:,1]= (lbl[:,:,1]-ymn)/(ymx-ymn)\n",
    "lbl[:,:,2]= (lbl[:,:,2]-xmn)/(xmx-xmn)\n",
    "lbl=cv2.bitwise_and(lbl,lbl,mask=msk)\n",
    "#lbl = cv2.resize(lbl, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "lbl = lbl.transpose(2, 0, 1)   # NHWC -> NCHW\n",
    "lbl = np.array(lbl, dtype=np.float64)\n",
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "type(trainimage_gt['warped_WC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.load(DATA_PATH + 'train/06/warped_document.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "image = Image.open(DATA_PATH + 'train/01/warped_document.png')\n",
    "# convert image to numpy array\n",
    "img = asarray(image)\n",
    "#img = m.imresize(img, self.img_size) # uint8 with RGB mode\n",
    "if img.shape[-1] == 4:\n",
    "    img=img[:,:,:3]   # Discard the alpha channel  \n",
    "img = img[:, :, ::-1] # RGB -> BGR\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "img = img.astype(float) / 255.0\n",
    "img = img.transpose(2, 0, 1) # NHWC -> NCHW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'Dataset Preview/Inv3D preview/data/'\n",
    "\n",
    "from custom_dataset import CustomImageDataset_wc\n",
    "train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=True)\n",
    "#train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=False)\n",
    "\n",
    "from models import unetnc\n",
    "model = unetnc.UnetGenerator(input_nc=3, output_nc=6, num_downs=3)\n",
    "#model.to('cuda')\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.parameters():\n",
    " #   param.requires_grad = False\n",
    "#for param in model.parameters():\n",
    "    #print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "#data_loader = get_loader('doc3dwc')\n",
    "#data_path = args.data_path\n",
    "#t_loader = data_loader(data_path, is_transform=True, img_size=(args.img_rows, args.img_cols), augmentations=True)\n",
    "#v_loader = data_loader(data_path, is_transform=True, split='val', img_size=(args.img_rows, args.img_cols))\n",
    "\n",
    "#n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=1, num_workers=8, shuffle=True)\n",
    "#valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, labels in trainloader:\n",
    "    #print(item)\n",
    "    pass\n",
    "#img = Variable(item['image'])#.to('cuda'))  #images = Variable(images.cuda())\n",
    "#img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'wc_gt': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'curvature_gt': ['TODO'],\n",
       " 'text_mask_gt': ['TODO'],\n",
       " 'local_angle_gt': ['TODO']}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    print(i)\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    #images = Variable(images.cuda())\n",
    "    #labels = Variable(labels.cuda())\n",
    "\n",
    "    #optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    '''pred=htan(outputs)\n",
    "    g_loss=gloss(pred, labels)\n",
    "    l1loss = loss_fn(pred, labels)\n",
    "    loss=l1loss#+(0.2*g_loss)\n",
    "    avg_l1loss+=float(l1loss)\n",
    "    avg_gloss+=float(g_loss)\n",
    "    avg_loss+=float(loss)\n",
    "    train_mse+=float(MSE(pred, labels).item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    global_step+=1'''\n",
    "    print('lakshooOOOO')\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Pytorch Lightning Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "DATA_PATH = 'Dataset Preview/Inv3D preview complete/data/'\n",
    "\n",
    "from custom_dataset import CustomImageDataset_wc\n",
    "dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=True)\n",
    "#train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=False)\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size= 4)\n",
    "\n",
    "\n",
    "from models import unetnc\n",
    "model = unetnc.UnetGenerator(input_nc=3, output_nc=7, num_downs=3)\n",
    "\n",
    "\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "#trainer.fit(model, train_loader)\n",
    "\n",
    "# Wie sieht der Batch aus, der vom trainer und train_loader generiert wird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "phi_xx = outputs[:,3:4,:,:]\n",
    "phi_xy = outputs[:,4:5,:,:]\n",
    "phi_yx = outputs[:,5:6,:,:]\n",
    "phi_yy = outputs[:,6:7,:,:]\n",
    "curvature_mesh = outputs[:,7:8,:,:]\n",
    "\n",
    "#l1_loss = torch.norm((wc_coordinates - labels['wc_gt']),p=1,dim=(1,2,3))\n",
    "\n",
    "theta_x = torch.atan2(phi_xx, phi_xy)\n",
    "theta_y = torch.atan2(phi_yx, phi_yy)\n",
    "#p_x = torch.norm((phi_xx, phi_xy),p=2,dim=(1,2,3))\n",
    "#p_y = torch.norm(phi_yx, phi_yy,p=2,dim=(1,2,3))\n",
    "theta_x_gt = labels['warped_angle_gt'][:,0:1,:,:]\n",
    "theta_y_gt = labels['warped_angle_gt'][:,1:2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_angle_def(theta_x, theta_y, theta_x_gt, theta_y_gt , type = 'paper'):\n",
    "    if type == 'paper':\n",
    "        l_x = (torch.abs(theta_x - theta_x_gt) - math.pi) % (2*math.pi)\n",
    "        l_y = (torch.abs(theta_y - theta_y_gt) - math.pi) % (2*math.pi)\n",
    "        l_angle = l_x + l_y\n",
    "        return l_angle\n",
    "    else:\n",
    "        l_x = (torch.abs(theta_x - theta_x_gt) - math.pi) % (math.pi)\n",
    "        l_y = (torch.abs(theta_y - theta_y_gt) - math.pi) % (math.pi)\n",
    "        l_angle = l_x + l_y\n",
    "        return l_angle\n",
    "l_angle = l_angle_def(theta_x, theta_y, theta_x_gt, theta_y_gt , type = 'paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 6.5502,  9.1036,  8.4098,  ..., 10.5725,  9.7732,  8.1592],\n",
       "          [ 9.5878, 11.1359, 10.6318,  ..., 10.5132,  8.8400,  8.1323],\n",
       "          [11.4941, 10.8336,  8.4940,  ...,  7.8338, 10.4742,  9.9351],\n",
       "          ...,\n",
       "          [10.0284,  7.5092,  9.4902,  ..., 10.1936,  9.6098,  9.5224],\n",
       "          [ 8.5193, 10.8087,  8.1074,  ..., 10.8176,  8.2405,  8.7833],\n",
       "          [ 9.2108, 11.6660,  8.8763,  ...,  9.3722,  8.8739,  7.8045]]],\n",
       "\n",
       "\n",
       "        [[[ 6.9675,  9.8282,  9.1158,  ...,  9.5963,  8.0754,  8.3313],\n",
       "          [ 9.7767, 11.5925, 10.3022,  ..., 11.7513, 11.6905,  9.9686],\n",
       "          [ 9.8845, 10.1646,  7.2149,  ...,  8.3453,  8.1525,  7.9435],\n",
       "          ...,\n",
       "          [11.1207, 10.0063,  9.9313,  ...,  9.5163, 10.6405,  9.8720],\n",
       "          [ 8.0595, 11.6377,  8.0761,  ..., 10.6131,  9.3898,  8.4291],\n",
       "          [ 9.1431, 10.9209, 10.3663,  ...,  8.9672,  8.9980,  7.8156]]],\n",
       "\n",
       "\n",
       "        [[[ 6.4337,  9.9325,  8.1331,  ...,  9.3034,  8.3387,  8.2734],\n",
       "          [ 9.9492, 11.7033, 10.5027,  ..., 11.1826, 10.4826, 10.0327],\n",
       "          [11.1712, 11.4449,  8.8253,  ...,  9.5154,  8.5905,  9.1628],\n",
       "          ...,\n",
       "          [11.0757, 10.7729,  9.9384,  ..., 10.2738, 10.8661, 10.8672],\n",
       "          [ 8.3209, 10.2393,  8.6333,  ...,  9.9984,  9.3384,  8.3548],\n",
       "          [ 9.0381, 10.7639, 10.9322,  ...,  9.6300,  9.1860,  7.6415]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "l_angle * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#import grad_loss\n",
    "# Losses\n",
    "#MSE = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "#gloss= grad_loss.Gradloss(window_size=5,padding=2)\n",
    "\n",
    "# Activation\n",
    "htan = nn.Hardtanh(0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensors(model_output):\n",
    "    batch_size = model_output.shape[0]\n",
    "    wc_coordinates_pred = torch.empty(batch_size,3,256,256)\n",
    "    #angle_pred = torch.empty(batch_size,4,256,256)\n",
    "    phi_xx = torch.empty(batch_size,1,256,256)\n",
    "    phi_xy = torch.empty(batch_size,1,256,256)\n",
    "    phi_yx = torch.empty(batch_size,1,256,256)\n",
    "    phi_yy = torch.empty(batch_size,1,256,256)\n",
    "\n",
    "    idx = 0\n",
    "    for output in model_output:\n",
    "        \n",
    "        wc_coordinates_pred[idx] = output[:3]\n",
    "        #angle_pred[idx] = output[3:]\n",
    "        phi_xx[idx] = output[3]\n",
    "        phi_xy[idx] = output[4]\n",
    "        phi_yx[idx] = output[5]\n",
    "        phi_yy[idx] = output[6]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    angle_pred = {'phi_xx' : phi_xx, 'phi_xy' : phi_xy, 'phi_yx' : phi_yx, 'phi_yy' : phi_yy,}\n",
    "    return wc_coordinates_pred, angle_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_coord = output[:,0:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.L1Loss()\n",
    "wc_coordinates_output, angle_output = split_tensors(output)\n",
    "\n",
    "#wc_coordinates_pred=htan(wc_coordinates_output)\n",
    "#l1_loss = loss_fn(wc_coordinates_pred, labels['wc_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = torch.cdist(wc_coordinates_output, labels['wc_gt'], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([105090.3906,  98479.5078, 102082.2266], grad_fn=<NormBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from torch import linalg as LA\n",
    "a = wc_coord - labels['wc_gt']\n",
    "torch.norm(a,p=1,dim=(1,2,3))\n",
    "#LA.matrix_norm(a, ord=1)\n",
    "\n",
    "#LA.norm(a, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = l1_loss #+ torch.tensor(0.25, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "#optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-141-291d24f22676>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  l1_loss.grad\n"
     ]
    }
   ],
   "source": [
    "l1_loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_x = torch.atan2(angle_output['phi_xx'], angle_output['phi_xy'])\n",
    "theta_y = torch.atan2(angle_output['phi_yx'], angle_output['phi_yy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[-2.3387,  1.8802, -3.0584,  ...,  2.3485, -3.0838,  1.8430],\n",
       "          [ 1.0988,  2.4272,  0.0600,  ...,  1.4874, -0.4773, -0.5839],\n",
       "          [ 1.1703,  2.1403,  0.9957,  ...,  1.9287,  0.8192, -2.4067],\n",
       "          ...,\n",
       "          [-0.9434,  3.1323, -0.2417,  ...,  2.9774,  0.6064, -2.6378],\n",
       "          [ 0.0099, -2.8343, -1.4282,  ...,  2.2990,  1.9182,  2.6005],\n",
       "          [ 1.0062, -3.1223,  0.5710,  ...,  2.4470,  0.3611, -0.6768]]],\n",
       "\n",
       "\n",
       "        [[[-2.5209,  1.1893, -2.5897,  ...,  2.3591,  2.8945,  1.1009],\n",
       "          [ 1.3222, -3.0580, -0.1832,  ...,  1.9137, -0.6337, -2.9021],\n",
       "          [ 2.5742,  2.0284,  1.1411,  ...,  2.2434,  0.8028, -2.9189],\n",
       "          ...,\n",
       "          [-0.7203,  2.6766, -0.1862,  ...,  2.5575,  0.6216, -2.8381],\n",
       "          [ 0.3242,  2.4139,  1.7511,  ...,  1.7789,  1.3775,  2.7694],\n",
       "          [ 0.9446,  2.9962,  0.5149,  ..., -2.4320,  0.1336, -1.1009]]],\n",
       "\n",
       "\n",
       "        [[[-2.3946,  1.2582, -2.7747,  ...,  2.3261,  2.8953,  1.0995],\n",
       "          [ 1.1989,  2.1980, -0.1703,  ...,  2.0578, -0.5005, -3.1247],\n",
       "          [ 2.4976,  2.1824,  0.9957,  ...,  2.0310,  0.7838,  2.1610],\n",
       "          ...,\n",
       "          [-0.3597,  2.5712, -0.2002,  ...,  2.5830,  0.3494, -2.7872],\n",
       "          [ 0.9157,  2.3296,  1.3602,  ...,  1.9379,  0.9922,  2.8097],\n",
       "          [ 0.8116,  2.7985,  0.7047,  ..., -1.9757,  0.1404,  0.2303]]]],\n",
       "       grad_fn=<Atan2Backward>)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "theta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x = torch.cdist(angle_output['phi_xx'],angle_output['phi_xy'])\n",
    "p_y = torch.cdist(angle_output['phi_yx'],angle_output['phi_yy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.cdist(angle_output['phi_xx'],angle_output['phi_xy'], p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 7.2391, 10.6285, 11.2850,  ..., 13.6684, 14.6986, 10.3504],\n",
       "          [ 7.2376,  8.2053,  9.3738,  ..., 11.4745, 12.9501,  9.9714],\n",
       "          [11.6378, 11.5819, 12.3489,  ..., 14.1231, 16.3125, 11.9589],\n",
       "          ...,\n",
       "          [ 6.5999,  8.1252,  8.6379,  ..., 10.4022, 12.0047,  7.9056],\n",
       "          [ 9.9543, 13.8107, 14.6640,  ..., 16.5764, 18.2547, 13.7567],\n",
       "          [ 7.8080,  8.8072, 10.2071,  ..., 11.5712, 14.3958,  9.3063]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1878, 10.7629, 10.9981,  ..., 13.6370, 12.2194,  6.3300],\n",
       "          [ 6.9235,  9.3275,  9.8757,  ..., 11.1887, 11.8747,  7.1685],\n",
       "          [12.0948, 12.1321, 13.3600,  ..., 14.5807, 14.1684,  8.3100],\n",
       "          ...,\n",
       "          [ 7.3444, 10.6796, 11.0797,  ..., 12.8764, 13.1144,  7.4377],\n",
       "          [12.4934, 12.7724, 14.0844,  ..., 15.8269, 14.5963,  8.6161],\n",
       "          [ 5.9442,  8.4455,  9.1291,  ..., 11.2682, 10.7131,  4.1938]]],\n",
       "\n",
       "\n",
       "        [[[ 7.6417, 12.6649, 10.1858,  ..., 14.9750, 11.8834,  6.0450],\n",
       "          [ 8.5360,  9.6211, 10.4425,  ..., 12.1824, 12.2683,  7.8784],\n",
       "          [12.3777, 13.7800, 12.1457,  ..., 15.8366, 13.0560,  8.1353],\n",
       "          ...,\n",
       "          [ 7.3246, 11.4149, 10.9665,  ..., 14.0200, 12.8707,  7.1935],\n",
       "          [12.8876, 14.2962, 13.0765,  ..., 16.5769, 14.2326,  8.3125],\n",
       "          [ 6.1729,  9.5545,  8.4015,  ..., 12.0493, 10.1930,  3.1494]]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'grad'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f7d35533bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "model.parameters().grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-a880a89f96e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.sum(l1_loss,torch.tensor(0.25, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.linear = nn.Linear(2,2)\n",
    "        nn.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.4341, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.8760, 1.4145]], requires_grad=True)\ntensor([[0.1553, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 2, requires_grad=True)\n",
    "out = net(input)\n",
    "print(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "emp = torch.empty(1,1)\n",
    "emp[0][0] = 2\n",
    "#emp[0][1] = 2'''\n",
    "\n",
    "emp2 = torch.empty(1,2)\n",
    "emp2[0][0] = 1\n",
    "emp2[0][1] = 2\n",
    "# Activation\n",
    "htan = nn.Hardtanh(0,1.0)\n",
    "t1 = torch.empty(1,1)\n",
    "t1[0] = out[0][0]\n",
    "t2 = torch.empty(1,1)\n",
    "t2[0] = out[0][1]\n",
    "#pred=htan(out)\n",
    "l1_loss = loss_fn((t1+2) * 5 , emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-299-291d24f22676>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  l1_loss.grad\n"
     ]
    }
   ],
   "source": [
    "l1_loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2555, 2.7904]])"
      ]
     },
     "metadata": {},
     "execution_count": 307
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}