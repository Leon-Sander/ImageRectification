{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd040211611db766dc04d4e34028715744ed9c5ebf71c9517b1ff2182e4424a3492",
   "display_name": "Python 3.8.8 64-bit ('rapids-0.19': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8ac587500089045bda0d8680467a78b2a5f7abf79b6086e96a7d36f8c178cf35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import unetnc\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "DATA_PATH = 'Dataset Preview/Inv3D preview/data/'\n",
    "# warped WC sind die 3D Koordinaten nach dem ersten Modell bzw. dewarpnet -> Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainimage_gt = np.load(DATA_PATH + 'train/06/warped_WC.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = trainimage_gt['warped_WC']\n",
    "#img_t = torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#normalize label\n",
    "lbl = img_t\n",
    "msk=((lbl[:,:,0]!=0)&(lbl[:,:,1]!=0)&(lbl[:,:,2]!=0)).astype(np.uint8)*255\n",
    "xmx, xmn, ymx, ymn,zmx, zmn= 1.2539363, -1.2442188, 1.2396319, -1.2289206, 0.6436657, -0.67492497   # calculate from all the wcs\n",
    "lbl[:,:,0]= (lbl[:,:,0]-zmn)/(zmx-zmn)\n",
    "lbl[:,:,1]= (lbl[:,:,1]-ymn)/(ymx-ymn)\n",
    "lbl[:,:,2]= (lbl[:,:,2]-xmn)/(xmx-xmn)\n",
    "lbl=cv2.bitwise_and(lbl,lbl,mask=msk)\n",
    "#lbl = cv2.resize(lbl, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "lbl = lbl.transpose(2, 0, 1)   # NHWC -> NCHW\n",
    "lbl = np.array(lbl, dtype=np.float64)\n",
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "type(trainimage_gt['warped_WC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-bc3e358d2bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train/06/warped_document.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    446\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "np.load(DATA_PATH + 'train/06/warped_document.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "image = Image.open(DATA_PATH + 'train/01/warped_document.png')\n",
    "# convert image to numpy array\n",
    "img = asarray(image)\n",
    "#img = m.imresize(img, self.img_size) # uint8 with RGB mode\n",
    "if img.shape[-1] == 4:\n",
    "    img=img[:,:,:3]   # Discard the alpha channel  \n",
    "img = img[:, :, ::-1] # RGB -> BGR\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "img = img.astype(float) / 255.0\n",
    "img = img.transpose(2, 0, 1) # NHWC -> NCHW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'Dataset Preview/Inv3D preview/data/'\n",
    "\n",
    "from custom_dataset import CustomImageDataset_wc\n",
    "train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=True)\n",
    "#train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=False)\n",
    "\n",
    "from models import unetnc\n",
    "model = unetnc.UnetGenerator(input_nc=3, output_nc=6, num_downs=3)\n",
    "#model.to('cuda')\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.parameters():\n",
    " #   param.requires_grad = False\n",
    "#for param in model.parameters():\n",
    "    #print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "#data_loader = get_loader('doc3dwc')\n",
    "#data_path = args.data_path\n",
    "#t_loader = data_loader(data_path, is_transform=True, img_size=(args.img_rows, args.img_cols), augmentations=True)\n",
    "#v_loader = data_loader(data_path, is_transform=True, split='val', img_size=(args.img_rows, args.img_cols))\n",
    "\n",
    "#n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=1, num_workers=8, shuffle=True)\n",
    "#valloader = data.DataLoader(v_loader, batch_size=args.batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, labels in trainloader:\n",
    "    #print(item)\n",
    "    pass\n",
    "#img = Variable(item['image'])#.to('cuda'))  #images = Variable(images.cuda())\n",
    "#img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'wc_gt': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'curvature_gt': ['TODO'],\n",
       " 'text_mask_gt': ['TODO'],\n",
       " 'local_angle_gt': ['TODO']}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    print(i)\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    #images = Variable(images.cuda())\n",
    "    #labels = Variable(labels.cuda())\n",
    "\n",
    "    #optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    '''pred=htan(outputs)\n",
    "    g_loss=gloss(pred, labels)\n",
    "    l1loss = loss_fn(pred, labels)\n",
    "    loss=l1loss#+(0.2*g_loss)\n",
    "    avg_l1loss+=float(l1loss)\n",
    "    avg_gloss+=float(g_loss)\n",
    "    avg_loss+=float(loss)\n",
    "    train_mse+=float(MSE(pred, labels).item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    global_step+=1'''\n",
    "    print('lakshooOOOO')\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Pytorch Lightning Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "DATA_PATH = 'Dataset Preview/Inv3D preview/data/'\n",
    "\n",
    "from custom_dataset import CustomImageDataset_wc\n",
    "dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=True)\n",
    "#train_dataset = CustomImageDataset_wc(data_dir=DATA_PATH+'train/', transform=False)\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size= 4)\n",
    "\n",
    "\n",
    "from models import unetnc\n",
    "model = unetnc.UnetGenerator(input_nc=3, output_nc=7, num_downs=3)\n",
    "\n",
    "\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "#trainer.fit(model, train_loader)\n",
    "\n",
    "# Wie sieht der Batch aus, der vom trainer und train_loader generiert wird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "\n",
    "output = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#import grad_loss\n",
    "# Losses\n",
    "#MSE = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "#gloss= grad_loss.Gradloss(window_size=5,padding=2)\n",
    "\n",
    "# Activation\n",
    "htan = nn.Hardtanh(0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensors(model_output):\n",
    "    batch_size = model_output.shape[0]\n",
    "    wc_coordinates_pred = torch.empty(batch_size,3,256,256)\n",
    "    #angle_pred = torch.empty(batch_size,4,256,256)\n",
    "    phi_xx = torch.empty(batch_size,1,256,256)\n",
    "    phi_xy = torch.empty(batch_size,1,256,256)\n",
    "    phi_yx = torch.empty(batch_size,1,256,256)\n",
    "    phi_yy = torch.empty(batch_size,1,256,256)\n",
    "\n",
    "    idx = 0\n",
    "    for output in model_output:\n",
    "        \n",
    "        wc_coordinates_pred[idx] = output[:3]\n",
    "        #angle_pred[idx] = output[3:]\n",
    "        phi_xx[idx] = output[3]\n",
    "        phi_xy[idx] = output[4]\n",
    "        phi_yx[idx] = output[5]\n",
    "        phi_yy[idx] = output[6]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    angle_pred = {'phi_xx' : phi_xx, 'phi_xy' : phi_xy, 'phi_yx' : phi_yx, 'phi_yy' : phi_yy,}\n",
    "    return wc_coordinates_pred, angle_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.L1Loss()\n",
    "wc_coordinates_output, angle_output = split_tensors(output)\n",
    "\n",
    "#wc_coordinates_pred=htan(wc_coordinates_output)\n",
    "#l1_loss = loss_fn(wc_coordinates_pred, labels['wc_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "angle_output['theta_xx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.2801, grad_fn=<L1LossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "l1_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = l1_loss #+ torch.tensor(0.25, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "#optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-141-291d24f22676>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  l1_loss.grad\n"
     ]
    }
   ],
   "source": [
    "l1_loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[-2.3387,  1.8802, -3.0584,  ...,  2.3485, -3.0838,  1.8430],\n",
       "          [ 1.0988,  2.4272,  0.0600,  ...,  1.4874, -0.4773, -0.5839],\n",
       "          [ 1.1703,  2.1403,  0.9957,  ...,  1.9287,  0.8192, -2.4067],\n",
       "          ...,\n",
       "          [-0.9434,  3.1323, -0.2417,  ...,  2.9774,  0.6064, -2.6378],\n",
       "          [ 0.0099, -2.8343, -1.4282,  ...,  2.2990,  1.9182,  2.6005],\n",
       "          [ 1.0062, -3.1223,  0.5710,  ...,  2.4470,  0.3611, -0.6768]]],\n",
       "\n",
       "\n",
       "        [[[-2.5209,  1.1893, -2.5897,  ...,  2.3591,  2.8945,  1.1009],\n",
       "          [ 1.3222, -3.0580, -0.1832,  ...,  1.9137, -0.6337, -2.9021],\n",
       "          [ 2.5742,  2.0284,  1.1411,  ...,  2.2434,  0.8028, -2.9189],\n",
       "          ...,\n",
       "          [-0.7203,  2.6766, -0.1862,  ...,  2.5575,  0.6216, -2.8381],\n",
       "          [ 0.3242,  2.4139,  1.7511,  ...,  1.7789,  1.3775,  2.7694],\n",
       "          [ 0.9446,  2.9962,  0.5149,  ..., -2.4320,  0.1336, -1.1009]]],\n",
       "\n",
       "\n",
       "        [[[-2.3946,  1.2582, -2.7747,  ...,  2.3261,  2.8953,  1.0995],\n",
       "          [ 1.1989,  2.1980, -0.1703,  ...,  2.0578, -0.5005, -3.1247],\n",
       "          [ 2.4976,  2.1824,  0.9957,  ...,  2.0310,  0.7838,  2.1610],\n",
       "          ...,\n",
       "          [-0.3597,  2.5712, -0.2002,  ...,  2.5830,  0.3494, -2.7872],\n",
       "          [ 0.9157,  2.3296,  1.3602,  ...,  1.9379,  0.9922,  2.8097],\n",
       "          [ 0.8116,  2.7985,  0.7047,  ..., -1.9757,  0.1404,  0.2303]]]],\n",
       "       grad_fn=<Atan2Backward>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "theta_x = torch.atan2(angle_output['phi_xx'], angle_output['phi_xy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'grad'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f7d35533bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "model.parameters().grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-a880a89f96e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.sum(l1_loss,torch.tensor(0.25, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.linear = nn.Linear(2,2)\n",
    "        nn.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.4341, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.8760, 1.4145]], requires_grad=True)\ntensor([[0.1553, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 2, requires_grad=True)\n",
    "out = net(input)\n",
    "print(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "emp = torch.empty(1,1)\n",
    "emp[0][0] = 2\n",
    "#emp[0][1] = 2'''\n",
    "\n",
    "emp2 = torch.empty(1,2)\n",
    "emp2[0][0] = 1\n",
    "emp2[0][1] = 2\n",
    "# Activation\n",
    "htan = nn.Hardtanh(0,1.0)\n",
    "t1 = torch.empty(1,1)\n",
    "t1[0] = out[0][0]\n",
    "t2 = torch.empty(1,1)\n",
    "t2[0] = out[0][1]\n",
    "#pred=htan(out)\n",
    "l1_loss = loss_fn((t1+2) * 5 , emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-299-291d24f22676>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  l1_loss.grad\n"
     ]
    }
   ],
   "source": [
    "l1_loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2555, 2.7904]])"
      ]
     },
     "metadata": {},
     "execution_count": 307
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}